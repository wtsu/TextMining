{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: I will use nltk to explore the Herman Melville novel Moby Dick. Then I will create a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step A - I count how many tokens (words and punctuation symbols) are in text1.\n",
    "\n",
    "*This function returns an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254989"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_a():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "step_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step B - I count how many unique tokens (unique words and punctuation) text1 has.\n",
    "\n",
    "*This function returns an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_b():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "step_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step C - After lemmatizing the verbs,  I count how many unique tokens text1 has.\n",
    "\n",
    "*This function returns an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16900"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def step_c():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "step_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - I determine the lexical diversity of the given text input. (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function returns a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08139566804842562"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_one():\n",
    "    \n",
    "    num_unique_tokens = len(set(text1))\n",
    "    num_total_tokens = len(text1)\n",
    "    \n",
    "    return num_unique_tokens/num_total_tokens\n",
    "\n",
    "step_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - I determine the percentage of tokens that is 'whale'or 'Whale'?\n",
    "\n",
    "*This function returns a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125668166077752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "def step_two():\n",
    "    dist = FreqDist(moby_tokens)\n",
    "    numofwhale = dist['whale']\n",
    "    numofWhale = dist['Whale']\n",
    "    num_total_tokens = sum(dist.values())\n",
    "    \n",
    "    return ((numofwhale+numofWhale)/num_total_tokens)*100\n",
    "\n",
    "step_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - I detemine the 20 most frequently occurring (unique) tokens in the text. I detemine their frequency.\n",
    "\n",
    "*This function returns a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list is sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2097),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_three():\n",
    "    \n",
    "    #sortedToken = sorted(list(set(moby_tokens)), key=lambda token: dist[token], reverse=True)\n",
    "    dist = FreqDist(moby_tokens)\n",
    "    #create a sorted list of the moby_tokens where the sorting is done based off the frequency each token occurs in \n",
    "    ##descending order\n",
    "    sortedToken = sorted(list(set(moby_tokens)), key=lambda x: dist[x], reverse = True)\n",
    "    answer = ([(x, dist[x]) for x in sortedToken[:20]])\n",
    "    return answer\n",
    "\n",
    "step_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - I determine which tokens have a length of greater than 5 and frequency of more than 150.\n",
    "\n",
    "*This function returns a sorted list of the tokens that match the above constraints. To sort list, I use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_four():\n",
    "    dist = FreqDist(moby_tokens)\n",
    "    #use a list comprehension to determine which tokens match the criteria\n",
    "    frequentTokens = [x for x in set(moby_tokens) if len(x)>5 and dist[x]>150]\n",
    "    #sort the list tokens that match the criteria\n",
    "    sortedTokens = sorted(frequentTokens)\n",
    "    return sortedTokens\n",
    "\n",
    "step_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - I find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function returns a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_five():\n",
    "    #determine which word is the longest using max\n",
    "    longestWord = max(text1, key=lambda x:len(x))\n",
    "\n",
    "    #create a tuple that combines the longest word with its length\n",
    "    x = longestWord, len(longestWord)\n",
    "    return x\n",
    "\n",
    "step_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 - I detemine what unique words have a frequency of more than 2000. amd their frequency?\n",
    "\n",
    "\"I use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function returns a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2097, 'I')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_six():\n",
    "    dist = FreqDist(moby_tokens)\n",
    "    #use a list comprehension to get words that are not punctuaation and occurs more than 2000 times\n",
    "    words = [x for x in set(text1) if x.isalpha() and dist[x]>2000]\n",
    "\n",
    "    #sort the words in descending order based off frequency of words\n",
    "    sortedwords = sorted(words,key=lambda word:dist[word], reverse=True)\n",
    "\n",
    "    #create a tuple in form of (frequency, word)\n",
    "    answer = [(dist[x], x) for x in sortedwords]\n",
    "\n",
    "    return answer\n",
    "\n",
    "step_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 - I determine the average number of tokens per sentence?\n",
    "\n",
    "*This function returns a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.881952902963864"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_seven():\n",
    "    #tokenize based of sentences\n",
    "    sentences = nltk.sent_tokenize(moby_raw)\n",
    "\n",
    "    #calculate the number of word tokens per sentence\n",
    "    numsOfTokens = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "    #answer is the total sum of all word tokens divided by the total number of sentences\n",
    "    answer = sum(numsOfTokens)/len(numsOfTokens)\n",
    "\n",
    "    return answer\n",
    "\n",
    "step_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 - I determine 5 most frequent parts of speech in this text and their frequency?\n",
    "\n",
    "*This function returns a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 32730), ('IN', 28657), ('DT', 25867), (',', 19204), ('JJ', 17620)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_eight():\n",
    "    #from the collections package, import the Counter object for quick tallies\n",
    "    from collections import Counter\n",
    "    counter = Counter()\n",
    "\n",
    "    #returns a list in the form of (word, part of speech)\n",
    "    tagged_tokens = nltk.pos_tag(moby_tokens)\n",
    "\n",
    "    #go through the tagged_token list, and whenever a certain part of speech occurs increase the tally of \n",
    "    ## the respective counter\n",
    "    for t in tagged_tokens:\n",
    "        #the counter is counting based of the POS that is in t[1]\n",
    "        counter[t[1]] += 1\n",
    "    #return the 5 most common counters\n",
    "    answer = counter.most_common(5)\n",
    "    return answer\n",
    "\n",
    "step_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Spelling Recommender\n",
    "\n",
    "I create three different spelling recommenders. Each takes a list of misspelled words and recommends a correctly spelled word for every word in the list.\n",
    "\n",
    "For every misspelled word, the recommender finds the word in `correct_spellings` that has the shortest distance*, and starts with the same letter as the misspelled word, and returns that word as a recommendation.\n",
    "\n",
    "*Each of the three different recommenders uses a different distance measure (outlined below).\n",
    "\n",
    "Each of the recommenders provide recommendations for the three default words provided: `['cormulent', 'incendenece', 'validrate']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "correct_spellings = words.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9 - For this recommender, I provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
    "\n",
    "*This function returns a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    entries=['cormulent', 'incendenece', 'validrate']\n",
    "    results = []\n",
    "    candidates = []\n",
    "    for entry in entries:\n",
    "    #find the word in the correct_spelling list that starts with the same first letter as the entries\n",
    "        candidates = [w for w in correct_spellings if w[0] == entry[0]]\n",
    "        #calculate the jaccard_distance between the entry and the candidate\n",
    "        ##find the candidate with the minimum jaccard_distance\n",
    "        results.append(min(candidates, key = lambda x:nltk.jaccard_distance(set(nltk.ngrams(entry,n =3)),\n",
    "                                                                           set(nltk.ngrams(x, n =3)))))\n",
    "    answer = results\n",
    "    return answer\n",
    "    \n",
    "step_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10 - for this recommender, I provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    results = []\n",
    "    for entry in entries:\n",
    "        #get the list of words in correct spelling whose first letter matches the entry word\n",
    "        candidates = [w for w in correct_spellings if w[0] == entry[0]]\n",
    "        \n",
    "        #find the word in correct spelling that has the minimum jaccard distance based of 4-grams\n",
    "        results.append(min(candidates, key=\n",
    "                       lambda x:nltk.jaccard_distance(set(nltk.ngrams(entry, n=4)), set(nltk.ngrams(x, n=4)))))\n",
    "    answer = results\n",
    "    return answer\n",
    "    \n",
    "step_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11 - For this recommender, I provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def step_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    results = []\n",
    "    for entry in entries:\n",
    "        #find the list of candidate words in correct spelling that share the same first letter as the entry\n",
    "        candidates = [w for w in correct_spellings if w[0] == entry[0]]\n",
    "        \n",
    "        #determine the word in correct spelling that has the minimum edit_distance\n",
    "        results.append(min(candidates, key=\n",
    "                           lambda x:nltk.edit_distance(entry, x)))\n",
    "    answers = results\n",
    "    \n",
    "    return answers\n",
    "    \n",
    "step_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
