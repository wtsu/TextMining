{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: I explore text message data and create models to predict if a message is spam or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_data = pd.read_csv('spam.csv')\n",
    "\n",
    "spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Determine the percentage of the documents in `spam_data` that are spam.\n",
    "\n",
    "*This function returns a float, the percent value (i.e. $ratio * 100$).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_one():\n",
    "    numofspam = len(spam_data[spam_data['target']==1])\n",
    "    totalnum = len(spam_data)\n",
    "    answer = (numofspam/totalnum)*100\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Fit the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "Determine the longest token in the vocabulary.\n",
    "\n",
    "*This function returns a string.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def step_two():\n",
    "    ##a count vectorizer does the following: Convert a collection of text documents to a matrix of token counts\n",
    "    #create the counter vectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    #Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    vect = vectorizer.fit(X_train)\n",
    "\n",
    "    #find the vocabulary words in the now trained vectorizer object with the longest length\n",
    "    answer = max(vect.get_feature_names(), key=lambda token:len(token))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.\n",
    "\n",
    "Next, fit a fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Find the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function  returns the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def step_three():\n",
    "    #create and train a countvectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    #Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    vect = vectorizer.fit(X_train)\n",
    "\n",
    "    #Transform documents to document-term matrix.\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "    #create a multinomial Naive Bayes Classifier with alpha = 0.1\n",
    "    clf = MultinomialNB(alpha=0.1)\n",
    "\n",
    "    #train the multinomial Naive Bayes Classifier, using the document-term matrix as training data\n",
    "    clf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    #using the created classifier predict results\n",
    "    ##create the document-term matrix of the test data to put into the trained NB classifier\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    predictions = clf.predict(X_test_vectorized)\n",
    "\n",
    "    #get the AUC score\n",
    "    ## AUC is the area under the ROC curve\n",
    "    ### ROC score tells us about how good the model can distinguish between two things\n",
    "    ### AUC closer to 1 means a good job of distinguishing the positive and the negative values\n",
    "    answer = roc_auc_score(y_test, predictions)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97208121827411165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer with default parameters.\n",
    "\n",
    "Determine the 20 features that have the smallest tf-idf and the 20 that have the largest tf-idf.\n",
    "\n",
    "Put these features in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series  be the feature name, and the data  be the tf-idf.\n",
    "\n",
    "The series of 20 features with smallest tf-idfs is sorted by smallest tfidf first, the list of 20 features with largest tf-idfs is sorted largest first. \n",
    "\n",
    "*This functions returns a tuple of two series\n",
    "`(smallest tf-idfs series, largest tf-idfs series)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def step_four():\n",
    "    #train the TfidVectorizer\n",
    "    ##tfidVectorizer is similar to countvectorizer except with TFIDFVectorizer \n",
    "    ##the value increases proportionally to count, but is offset by the \n",
    "    ##frequency of the word in the corpus. - This is the IDF (inverse document frequency part).\n",
    "    vect = TfidfVectorizer().fit(X_train)\n",
    "\n",
    "    #Transform documents to document-term matrix.\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "    #get the vocabulary from the TfidVectorizer and store in an numpy array\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "    #for each feature, get the maximum tfidf_index and then return a sorted array(but with indexes and in ascending order)\n",
    "    sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "\n",
    "    #get the features that correspond with the first 20 elements of the sorted index(and therefore the 20 features with the\n",
    "    ##smallest Tfid values)\n",
    "    small_index = feature_names[sorted_tfidf_index[:20]]\n",
    "\n",
    "    #get the tfid index value for those 20 features\n",
    "    small_value = X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[:20]]\n",
    "\n",
    "    #create a tuple combining the feature with it's Tfid value\n",
    "    smallTuples = [(value, word) for word, value in zip(small_index, small_value)]\n",
    "\n",
    "    #sort the tuples\n",
    "    smallTuples.sort()\n",
    "\n",
    "    #create two seperate arrays to store the index of the smallest values and the actual value of the smallest values\n",
    "    small_index = [element[1] for element in smallTuples]\n",
    "    small_value = [element[0] for element in smallTuples]\n",
    "\n",
    "    #create a pandas df with the two arrays just created\n",
    "    small_series = pd.Series(small_value,index=small_index)\n",
    "\n",
    "    #Determine the features with the biggest Tfidf values\n",
    "    big_index = feature_names[sorted_tfidf_index[-20:]]\n",
    "    big_value = X_train_vectorized.max(0).toarray()[0][sorted_tfidf_index[-20:]]\n",
    "\n",
    "    #set value as negative so that sort() sorts it correctly\n",
    "    ##.sort() sorts in ascending order, but we want descending order\n",
    "    bigTuples = [(-value,word) for word,value in zip(big_index, big_value)]\n",
    "    bigTuples.sort()\n",
    "    big_index = [element[1] for element in bigTuples]\n",
    "\n",
    "    #apply another negative so that that value is converted back to positive\n",
    "    big_value = [-element[0] for element in bigTuples]\n",
    "    big_series = pd.Series(big_value, index = big_index)\n",
    "\n",
    "    #merge the two df together to create the solution\n",
    "    a = (small_series, big_series)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(aaniye          0.074475\n",
       " athletic        0.074475\n",
       " chef            0.074475\n",
       " companion       0.074475\n",
       " courageous      0.074475\n",
       " dependable      0.074475\n",
       " determined      0.074475\n",
       " exterminator    0.074475\n",
       " healer          0.074475\n",
       " listener        0.074475\n",
       " organizer       0.074475\n",
       " pest            0.074475\n",
       " psychiatrist    0.074475\n",
       " psychologist    0.074475\n",
       " pudunga         0.074475\n",
       " stylist         0.074475\n",
       " sympathetic     0.074475\n",
       " venaam          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64, 146tf150p    1.000000\n",
       " 645          1.000000\n",
       " anything     1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " done         1.000000\n",
       " er           1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " ok           1.000000\n",
       " okie         1.000000\n",
       " thank        1.000000\n",
       " thanx        1.000000\n",
       " too          1.000000\n",
       " where        1.000000\n",
       " yup          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **3**.\n",
    "\n",
    "Then fit a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function returns the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_five():\n",
    "    #fit the training data using the Tfidf Vectorizer ignoring terms that have a document frequency lower than 3\n",
    "    ## Document frequency is the number of times a given term or query appears in a specific document\n",
    "    vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "\n",
    "    #transform the training data\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "    #create a multinomial NB with alpha = 0.1\n",
    "    model = MultinomialNB(alpha=0.1)\n",
    "\n",
    "    #fit a multinomial NB with alpha = 0.1 with the transformed train data\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    #predict labels using the trained NB classifier, using the transformed test data\n",
    "    predictions = model.predict(vect.transform(X_test))\n",
    "\n",
    "    #return AUC score\n",
    "    answer = roc_auc_score(y_test, predictions)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94162436548223349"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "Determine the average length of documents (number of characters) for not spam and spam documents?\n",
    "\n",
    "*This function returns a tuple (average length not spam, average length spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_six():\n",
    "    #create a column in the spam_data df that is just the # of characters\n",
    "    spam_data['length'] = spam_data['text'].str.len()\n",
    "\n",
    "    #create a nonSpam df from the spam_data df \n",
    "    nonSpam = spam_data[spam_data['target'] == 0]\n",
    "\n",
    "    #create a spam df from the spam_data df\n",
    "    spam = spam_data[spam_data['target'] == 1]\n",
    "\n",
    "    #calculate the average length of non spam text\n",
    "    num_of_nonspam = nonSpam['length'].sum()\n",
    "    totalnum_of_nonspam = len(nonSpam)\n",
    "\n",
    "    #calculate the average lenght of spam text\n",
    "    num_of_spam = spam['length'].sum()\n",
    "    totalnum_of_spam = len(spam)\n",
    "\n",
    "    answer = (num_of_nonspam/totalnum_of_nonspam, num_of_spam/totalnum_of_spam)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "This function helps combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7\n",
    "\n",
    "Fit and transform the training data X_train using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5**.\n",
    "\n",
    "Using this document-term matrix add an additional feature, **the length of document (number of characters)**, fit a Support Vector Classification model with regularization `C=10000`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function returns the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def step_seven():\n",
    "    from sklearn.svm import SVC\n",
    "    #create a Tfidf Vectorizer that ignores terms that have document frequency lower than 5 and train it off training data\n",
    "    vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "\n",
    "    #transform the vectorizer into a document matrix\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "    #get the length of each text in the test data\n",
    "    a=X_train.str.len()\n",
    "\n",
    "    #add feature to the document matrix\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, a)\n",
    "\n",
    "    #create a document matrix of test data\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "    #get the length of each text in the test data\n",
    "    b=X_test.str.len()\n",
    "\n",
    "    #add the feature to the test document matrix\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, b)\n",
    "\n",
    "    #create the support vector machine model\n",
    "    model = SVC(C=10000)\n",
    "\n",
    "    #train the support vector machine model\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    #get the predicted labels of the support vector machine base of the document matrix\n",
    "    predictions = model.predict(X_test_vectorized) \n",
    "\n",
    "    #get the AUC score\n",
    "    answer = roc_auc_score(y_test, predictions)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95813668234215565"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_seven()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8\n",
    "\n",
    "Determine the average number of digits per document for not spam and spam documents?\n",
    "\n",
    "*This function returns a tuple (average # digits not spam, average # digits spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_eight():\n",
    "    #import the regular expression package\n",
    "    import re\n",
    "\n",
    "    #go through all the spam text and find all the digits\n",
    "    spam = [re.findall(\"\\d\",i) for i in spam_data['text'][spam_data.target==1]]\n",
    "\n",
    "    #go through all the non spam text and find all the digits\n",
    "    non_spam = [re.findall(\"\\d\",i) for i in spam_data['text'][spam_data.target==0]]\n",
    "\n",
    "    #apply the len function to all digits to get the number of digits in nonspam \n",
    "    #and convert to a list that numpy can calculate the mean of\n",
    "    avg_num_digits_nonspam = np.mean(list(map(len,non_spam)))\n",
    "\n",
    "    #apply the len function to all digits to get the number of digits in spam \n",
    "    #and convert to a list that numpy can calculate the mean of\n",
    "    avg_num_digits_spam = np.mean(list(map(len,spam)))                   \n",
    "\n",
    "    answer = (avg_num_digits_nonspam,avg_num_digits_spam)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29927461139896372, 15.759036144578314)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_eight()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9\n",
    "\n",
    "Fit and transform the training data `X_train` using a Tfidf Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams).\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* **number of digits per document**\n",
    "\n",
    "fit a Logistic Regression model with regularization `C=100`. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "*This function returns the AUC score as a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def step_nine():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import re\n",
    "\n",
    "    #Fit a Tfidf Vectorizer ignoring document frequency lower 5 and using n-grams from n=1 to n=3\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "\n",
    "    #transform Tfidf Vectorizer for X_train\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "    #add training text length\n",
    "    a = X_train.str.len()\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, a)\n",
    "\n",
    "    #in training data find all digits\n",
    "    X_train_digits = X_train.str.findall(r'(\\d)')\n",
    "\n",
    "    #add number of digits in training text as a feature\n",
    "    num_of_digits = list(map(len, X_train_digits))\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, num_of_digits)\n",
    "\n",
    "    #transform Tfidf Vectorizer for X_test\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "    #add test text length\n",
    "    b=X_test.str.len()\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, b)\n",
    "\n",
    "    #add number of digits in test text as a feature\n",
    "    X_test_digits = X_test.str.findall(r'(\\d)')\n",
    "    num_of_digits = list(map(len, X_test_digits))\n",
    "    X_test_vectorized = add_feature(X_test_vectorized,num_of_digits )\n",
    "\n",
    "    #create logistic regression classifier\n",
    "    model = LogisticRegression(C=100)\n",
    "\n",
    "    #fit logistic regression classifier with document matrix train data with added features\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    #get predicted labels\n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "\n",
    "    #get the ROC\n",
    "    answer = roc_auc_score(y_test, predictions)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96533283533945646"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10\n",
    "\n",
    "Determine the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents. Use `\\w` and `\\W` character classes\n",
    "\n",
    "*This function returns a tuple (average # non-word characters not spam, average # non-word characters spam).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_ten():\n",
    "    import re\n",
    "    #find all the words in the spam data\n",
    "    spam = [re.findall(\"\\W\",i) for i in spam_data['text'][spam_data.target==1]]\n",
    "    \n",
    "    #find all the words in the non spam data\n",
    "    non_spam = [re.findall(\"\\W\",i) for i in spam_data['text'][spam_data.target==0]]\n",
    "\n",
    "    #calculate the average number of nonspam words\n",
    "    avg_num_of_nonspam_words = np.mean(list(map(len,non_spam)))\n",
    "    avg_num_of_spam_words = np.mean(list(map(len,spam)))\n",
    "    answer = (avg_num_of_nonspam_words,avg_num_of_spam_words)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.291813471502589, 29.041499330655956)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11\n",
    "\n",
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "To tell Count Vectorizer to use character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This  make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this document-term matrix and the following additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* **number of non-word characters (anything other than a letter, digit or underscore.)**\n",
    "\n",
    "fit a Logistic Regression model with regularization C=100. Then compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Also **find the 10 smallest and 10 largest coefficients from the model** and return them along with the AUC score in a tuple.\n",
    "\n",
    "The list of 10 smallest coefficients  be sorted smallest first, the list of 10 largest coefficients  be sorted largest first.\n",
    "\n",
    "The three features that were added to the document term matrix  have the following names  they appear in the list of coefficients:\n",
    "['length_of_doc', 'digit_count', 'non_word_char_count']\n",
    "\n",
    "*This function returns a tuple `(AUC score as a float, smallest coefs list, largest coefs list)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_eleven():\n",
    "    #train Count Vectorizer ignoring terms with document frequency lower than 5 and using n-grams from n=2 to n=5\n",
    "    vect = CountVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb').fit(X_train)\n",
    "\n",
    "    #transform to document matrix\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "    #add feature that is length of text\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, X_train.str.len())\n",
    "\n",
    "    #add feature that is number of digits\n",
    "    X_train_digits = X_train.str.findall(r'(\\d)')\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, list(map(len, X_train_digits)))\n",
    "\n",
    "    #add feature that is number of words\n",
    "    X_train_nonChar = X_train.str.findall(r'(\\W)')\n",
    "    X_train_vectorized = add_feature(X_train_vectorized, list(map(len, X_train_nonChar)))\n",
    "\n",
    "    #create document matrix of text data\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "\n",
    "    #add feature that is length of text\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, X_test.str.len())\n",
    "\n",
    "    #add feature that is number of digits \n",
    "    X_test_digits = X_test.str.findall(r'(\\d)')\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, list(map(len, X_test_digits)))\n",
    "\n",
    "    #add feature that is number of words\n",
    "    X_test_nonChar = X_test.str.findall(r'(\\W)')\n",
    "    X_test_vectorized = add_feature(X_test_vectorized, list(map(len, X_test_nonChar)))\n",
    "\n",
    "    #create logistic regression classifier\n",
    "    model = LogisticRegression(C=100)\n",
    "\n",
    "    #train the logistic regression classifier\n",
    "    model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "    #create prediction \n",
    "    predictions = model.predict(X_test_vectorized)\n",
    "\n",
    "\n",
    "    #find the 10 smallest and 10 largest coefficients from the model\n",
    "    #get all the feature names\n",
    "    feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "    #add to the list of feature names the 'length_of_doc', 'digit_count', 'non_word_char_count'\n",
    "    ##these are features that were later added\n",
    "    feature_names = np.concatenate((feature_names, np.array(['length_of_doc', 'digit_count', 'non_word_char_count'])))\n",
    "\n",
    "    #get a sorted array(by index) of logistic coefficients\n",
    "    sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "    #the list of smallest coefficients by index in ascending order\n",
    "    small_coefficient = list(feature_names[sorted_coef_index[:10]])\n",
    "\n",
    "    #the list of largest coefficients by index in descending order\n",
    "    large_coefficient = list(feature_names[sorted_coef_index[:-11:-1]])\n",
    "\n",
    "    #get AUC score\n",
    "    auc_score = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    #create a tuple that is the answer\n",
    "    answer = (auc_score,small_coefficient,large_coefficient)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.97885931107074342,\n",
       " ['. ', '..', '? ', ' i', ' y', ' go', ':)', ' h', 'go', ' m'],\n",
       " ['digit_count', 'ne', 'ia', 'co', 'xt', ' ch', 'mob', ' x', 'ww', 'ar'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
