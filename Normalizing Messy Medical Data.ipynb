{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Normalize Messy Medical Data using Regular Expressions\n",
    "\n",
    "Each line of the `dates.txt` file corresponds to a medical note. Each note has a date that needs to be extracted, but each date is encoded in one of many formats.\n",
    "\n",
    "I will be identifing all of the different date variants encoded in this dataset. I will then properly normalize and sort the medical notes by ascending chronological order. \n",
    "\n",
    "*This function return a Series of length 500 and dtype int.*\n",
    "This pandas Series returns the original Series' indices in chronological order\n",
    "\n",
    "For example if the original series was this:\n",
    "\n",
    "    0    1999\n",
    "    1    2010\n",
    "    2    1978\n",
    "    3    2015\n",
    "    4    1985\n",
    "\n",
    "The function would return this:\n",
    "\n",
    "    0    2\n",
    "    1    4\n",
    "    2    0\n",
    "    3    1\n",
    "    4    3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         03/25/93 Total time of visit (in minutes):\\n\n",
       "1                       6/18/85 Primary Care Doctor:\\n\n",
       "2    sshe plans to move as of 7/8/71 In-Home Servic...\n",
       "3                7 on 9/27/75 Audit C Score Current:\\n\n",
       "4    2/6/96 sleep studyPain Treatment Pain Level (N...\n",
       "5                    .Per 7/06/79 Movement D/O note:\\n\n",
       "6    4, 5/18/78 Patient's thoughts about current su...\n",
       "7    10/24/89 CPT Code: 90801 - Psychiatric Diagnos...\n",
       "8                         3/7/86 SOS-10 Total Score:\\n\n",
       "9             (4/10/71)Score-1Audit C Score Current:\\n\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load the 'dates.txt' file. \n",
    "doc = []\n",
    "\n",
    "#reads each line of the dates.txt file and then adds each new read line as another element in the 'doc' array\n",
    "with open('dates.txt') as file:\n",
    "    for line in file:\n",
    "        doc.append(line)\n",
    "\n",
    "df = pd.Series(doc)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     9\n",
       "1    84\n",
       "2     2\n",
       "3    53\n",
       "4    28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def date_sorter():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    \n",
    "    #use various regex's to account for all the different ways a date may be encoded\n",
    "    #Here is a list the variants I would encounter in this dataset:\n",
    "        #* 04/20/2009; 04/20/09; 4/20/09; 4/3/09\n",
    "        #* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009; \n",
    "        #* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009 \n",
    "        #* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009 \n",
    "        #* Feb 2009; Sep 2009; Oct 2010\n",
    "        #* 6/2008; 12/2009\n",
    "        #* 2009; 2010\n",
    "        \n",
    "    #regex matches with: 04/20/09; 4/20/09; 4/3/09\n",
    "    a1_a =df.str.extractall(r'(\\d{1,2})[\\/-](\\d{1,2})[\\/-](\\d{2})\\b')\n",
    "    #regex matches with: 04/20/2009\n",
    "    a1_b =df.str.extractall(r'(\\d{1,2})[\\/-](\\d{1,2})[\\/-](\\d{4})\\b')\n",
    "    a1 = pd.concat([a1_a,a1_b])\n",
    "    a1.reset_index(inplace=True)\n",
    "    a1_index = a1['level_0']\n",
    "\n",
    "    #regex matches with: Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009\n",
    "    a2 = df.str.extractall(r'((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[-.]* )((?:\\d{1,2}[?:, -]*)\\d{4})')\n",
    "    a2.reset_index(inplace=True)\n",
    "    a2_index = a2['level_0']\n",
    "\n",
    "    #regex matches with:  20 Mar 2009; 20 March 2009;  20 March, 2009; Feb 2009; Sep 2009; Oct 2010\n",
    "    a3 = df.str.extractall(r'((?:\\d{1,2} ))?((?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[?:, -]* )(\\d{4})')\n",
    "    a3.reset_index(inplace=True)\n",
    "    a3_index = a3['level_0']\n",
    "\n",
    "    #regex matches with: 04/20/2009(overlaps with a1); 6/2008; 12/2009\n",
    "    a4 = df.str.extractall(r'(\\d{1,2})[\\/](\\d{4})')\n",
    "    a4.reset_index(inplace=True)\n",
    "    a4_index = a4['level_0']\n",
    "\n",
    "    #strings matched by the a4 pattern are also captured by the a1 pattern.\n",
    "    #I want to only isolate the a4 matches that do not overlap with the a1 matches\n",
    "    save=[]\n",
    "    for i in a4_index:\n",
    "        if not(i in a1_index.values):\n",
    "             save.append(i)\n",
    "    save = np.asarray(save)\n",
    "    a4 = a4[a4['level_0'].isin(save)]\n",
    "    \n",
    "    #regex matches with: \n",
    "        #* 04/20/2009;\n",
    "        #* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009;\n",
    "        #* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "        #* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "        #* Feb 2009; Sep 2009; Oct 2010\n",
    "        #* 6/2008; 12/2009\n",
    "        #* 2009; 2010 \n",
    "    a5_a= df.str.extractall(r'[a-z]?[^0-9](\\d{4})[^0-9]')\n",
    "    \n",
    "    #regex matches with:\n",
    "        #* 04/20/2009;\n",
    "        #* Mar-20-2009; Mar 20, 2009; March 20, 2009;  Mar. 20, 2009; Mar 20 2009\n",
    "        #* 20 Mar 2009; 20 March 2009; 20 Mar. 2009; 20 March, 2009\n",
    "        #* Mar 20th, 2009; Mar 21st, 2009; Mar 22nd, 2009\n",
    "        #* Feb 2009; Sep 2009; Oct 2010\n",
    "        #* 6/2008; 12/2009\n",
    "        #* 2009; 2010 \n",
    "    a5_b = df.str.extractall(r'^(\\d{4})[^0-9]')\n",
    "    a5 = pd.concat([a5_a,a5_b])\n",
    "    a5.reset_index(inplace=True)\n",
    "    a5_index = a5['level_0']\n",
    "\n",
    "    #regex matches in a5 overlaps with regex matches in a2, a3, and a4.\n",
    "    #I only want the a5 index matchs that do not overlap with a2,a3, or a4\n",
    "    save=[]\n",
    "    for i in a5_index:\n",
    "        if not((i in a2_index.values) | (i in a3_index.values) | (i in a4_index.values)):\n",
    "             save.append(i)\n",
    "    save = np.asarray(save)\n",
    "    a5 = a5[a5['level_0'].isin(save)]\n",
    "\n",
    "    #each line is now matched with the correct date variant\n",
    "    #I create additional columns containing the month, day, and year isolated\n",
    "    a1.columns = ['level_0', 'match', 'month', 'day', 'year']\n",
    "    a1['year'] = a1['year'].apply(str)\n",
    "    a1['year'] = a1['year'].apply(lambda x: '19'+x if len(x)<=2 else x)\n",
    "\n",
    "    a2[1] = a2[1].apply(lambda x:x.replace(',',''))\n",
    "    a2['day'] = a2[1].apply(lambda x:x.split(' ')[0])\n",
    "    a2['year'] = a2[1].apply(lambda x:x.split(' ')[1])\n",
    "    a2 = a2.drop(1, axis = 1)\n",
    "    a2.columns = ['level_0', 'match', 'month', 'day', 'year']\n",
    "    a2['month'] = a2.month.apply(lambda x: x[:3])\n",
    "    a2['month'] = pd.to_datetime(a2.month, format='%b').dt.month\n",
    "\n",
    "    a3.columns = ['level_0', 'match', 'day', 'month', 'year']\n",
    "    a3['day']=a3['day'].replace(np.nan,1)\n",
    "    a3['month'] = a3.month.apply(lambda x: x[:3])\n",
    "    a3['month'] = pd.to_datetime(a3.month, format='%b').dt.month\n",
    "\n",
    "    a4.columns = ['level_0','match','month','year']\n",
    "    a4['day'] = 1\n",
    "\n",
    "    a5.columns=['level_0','match','year']\n",
    "    a5['day']=1\n",
    "    a5['month']=1\n",
    "    \n",
    "    #combine all the normalized strings together\n",
    "    results = pd.concat([a1,a2,a3,a4,a5])\n",
    "    results['date'] =pd.to_datetime(results['month'].apply(str)+'/'+results['day'].apply(str)+'/'+results['year'].apply(str))\n",
    "    results = results.sort_values(by = 'level_0')\n",
    "    results = results.set_index('level_0')\n",
    "    final = results['date']\n",
    "\n",
    "    n = np.arange(500)\n",
    "    sorted_dates = sorted(enumerate(final),key=lambda x:x[1])\n",
    "\n",
    "    return pd.Series(list(i[0] for i in sorted_dates),n)\n",
    "\n",
    "date_sorter().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "LvcWI",
   "launcher_item_id": "krne9",
   "part_id": "Mkp1I"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "534px",
    "left": "328px",
    "right": "152px",
    "top": "130px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
